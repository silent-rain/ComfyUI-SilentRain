# llama-flow

`llama-flow` æ˜¯ä¸€ä¸ªåŸºäº [llama-cpp-2](https://github.com/utilityai/llama-cpp-rs) æ„å»ºçš„ Rust é«˜æ€§èƒ½æœ¬åœ°æ¨ç†å¼•æ“ï¼Œä¸“ä¸ºç”Ÿäº§ç¯å¢ƒè®¾è®¡ã€‚å®ƒæä¾›äº†å®Œæ•´çš„ OpenAI Chat Completion API å…¼å®¹æ¥å£ï¼Œæ”¯æŒçº¯æ–‡æœ¬å’Œå¤šæ¨¡æ€ï¼ˆè§†è§‰ï¼‰æ¨ç†ï¼Œè®©ä½ èƒ½å¤Ÿåœ¨æœ¬åœ°è®¾å¤‡ä¸Šè¿è¡Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œæ— éœ€ä¾èµ–äº‘æœåŠ¡ã€‚

**æ ¸å¿ƒä¼˜åŠ¿ï¼š**

- ğŸš€ **åŸç”Ÿæ€§èƒ½**ï¼šåˆ©ç”¨ Rust çš„é›¶æˆæœ¬æŠ½è±¡å’Œ llama.cpp çš„ C++ ä¼˜åŒ–ï¼Œæä¾›æ¥è¿‘åŸç”Ÿçš„æ¨ç†é€Ÿåº¦
- ğŸ”Œ **å³æ’å³ç”¨**ï¼šå®Œå…¨å…¼å®¹ OpenAI API æ ‡å‡†
- ğŸ¯ **ç”Ÿäº§å°±ç»ª**ï¼šå†…ç½®æ¨¡å‹ç¼“å­˜ã€ä¼šè¯ç®¡ç†ã€é”™è¯¯å¤„ç†ç­‰ç‰¹æ€§
- ğŸŒ **å¤šæ¨¡æ€æ”¯æŒ**ï¼šæ”¯æŒè§†è§‰-è¯­è¨€æ¨¡å‹
- âš¡ **GPU åŠ é€Ÿ**ï¼šæ”¯æŒ CUDAï¼ˆNVIDIAï¼‰å’Œ Vulkanï¼ˆAMD/Intel/ç§»åŠ¨è®¾å¤‡ï¼‰
- ğŸ”„ **æµå¼å“åº”**ï¼šæ”¯æŒ Server-Sent Events (SSE) æµå¼è¾“å‡º
- ğŸ›¡ï¸ **ç±»å‹å®‰å…¨**ï¼šåˆ©ç”¨ Rust çš„ç±»å‹ç³»ç»Ÿæä¾›ç¼–è¯‘æ—¶å®‰å…¨ä¿éšœ
- ğŸ“¦ **è½»é‡éƒ¨ç½²**ï¼šæ”¯æŒ Androidã€iOSã€Linuxã€Windowsã€macOS ç­‰å¤šå¹³å°

## ç‰¹æ€§

- **çº¯æ–‡æœ¬æ¨ç†**ï¼šæ”¯æŒ GGUF æ ¼å¼æ¨¡å‹çš„æ–‡æœ¬ç”Ÿæˆ
- **å¤šæ¨¡æ€æ”¯æŒ**ï¼šé€šè¿‡ mmproj æ–‡ä»¶æ”¯æŒè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ Qwen3-VLï¼‰
- **å¼‚æ­¥ API**ï¼šåŸºäº Tokio çš„å¼‚æ­¥æ¨ç†æ¥å£
- **OpenAI API å…¼å®¹**ï¼šå®Œå…¨å…¼å®¹ OpenAI Chat Completion API æ ‡å‡†ï¼ˆ`async-openai`ï¼‰
- **æ¨¡å‹ç¼“å­˜**ï¼šå†…ç½®å…¨å±€ç¼“å­˜ç®¡ç†å™¨ï¼ˆåŸºäº DashMapï¼‰ï¼Œæ”¯æŒå¤šä¼šè¯æ¨¡å‹å¤ç”¨
- **ä¼šè¯ç®¡ç†**ï¼šæ”¯æŒå¤šä¼šè¯ä¸Šä¸‹æ–‡éš”ç¦»å’Œå†å²æ¶ˆæ¯ç®¡ç†
- **é’©å­ç³»ç»Ÿ**ï¼šçµæ´»çš„æ¨ç†ç”Ÿå‘½å‘¨æœŸé’©å­ï¼ˆæ¶ˆæ¯éªŒè¯ã€å†å²åŠ è½½ã€é”™è¯¯å¤„ç†ç­‰ï¼‰
- **æµå¼å“åº”**ï¼šæ”¯æŒæµå¼å’Œéæµå¼ä¸¤ç§è¾“å‡ºæ¨¡å¼
- **çµæ´»é‡‡æ ·**ï¼šæ”¯æŒ temperatureã€top_kã€top_pã€presence_penaltyã€frequency_penalty ç­‰å‚æ•°
- **è·¨å¹³å° GPU åŠ é€Ÿ**ï¼šæ”¯æŒ CUDAï¼ˆNVIDIAï¼‰å’Œ Vulkanï¼ˆAMD/Intel/ç§»åŠ¨è®¾å¤‡ï¼‰

## æ¶æ„

```sh
llama-flow/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ lib.rs                 # åº“å…¥å£ï¼Œå¯¼å‡ºä¸»è¦ç±»å‹
â”‚   â”œâ”€â”€ pipeline/              # æ¨ç†æµæ°´çº¿ï¼ˆæ ¸å¿ƒ APIï¼‰
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ pipeline_config.rs # æµæ°´çº¿é…ç½®
â”‚   â”‚   â””â”€â”€ pipeline_impl.rs   # æ¨ç†å®ç°
â”‚   â”œâ”€â”€ context.rs             # æ–‡æœ¬ä¸Šä¸‹æ–‡ç®¡ç†
â”‚   â”œâ”€â”€ mtmd_context.rs        # å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ç®¡ç†
â”‚   â”œâ”€â”€ model.rs               # æ¨¡å‹åŠ è½½ä¸ç®¡ç†
â”‚   â”œâ”€â”€ backend.rs             # llama.cpp åç«¯åˆå§‹åŒ–
â”‚   â”œâ”€â”€ sampler.rs             # é‡‡æ ·å™¨é…ç½®
â”‚   â”œâ”€â”€ cache.rs               # å…¨å±€æ¨¡å‹ç¼“å­˜ç®¡ç†
â”‚   â”œâ”€â”€ history/               # èŠå¤©å†å²ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ manager.rs         # å†å²ç®¡ç†å™¨
â”‚   â”‚   â”œâ”€â”€ session.rs         # ä¼šè¯ä¸Šä¸‹æ–‡
â”‚   â”‚   â””â”€â”€ mod.rs
â”‚   â”œâ”€â”€ hooks/                 # æ¨ç†é’©å­ç³»ç»Ÿ
â”‚   â”‚   â”œâ”€â”€ traits.rs          # é’©å­æ¥å£å®šä¹‰
â”‚   â”‚   â”œâ”€â”€ registry.rs        # é’©å­æ³¨å†Œå™¨
â”‚   â”‚   â”œâ”€â”€ context.rs         # é’©å­ä¸Šä¸‹æ–‡
â”‚   â”‚   â”œâ”€â”€ pipeline_state.rs  # æµæ°´çº¿çŠ¶æ€ç®¡ç†
â”‚   â”‚   â””â”€â”€ builtin/           # å†…ç½®é’©å­
â”‚   â”‚       â”œâ”€â”€ validate.rs    # è¯·æ±‚éªŒè¯
â”‚   â”‚       â”œâ”€â”€ normalize.rs   # æ¶ˆæ¯å½’ä¸€åŒ–
â”‚   â”‚       â”œâ”€â”€ system_prompt.rs # ç³»ç»Ÿæç¤ºè¯å¤„ç†
â”‚   â”‚       â”œâ”€â”€ load_history.rs  # åŠ è½½å†å²æ¶ˆæ¯
â”‚   â”‚       â”œâ”€â”€ assemble_messages.rs # æ¶ˆæ¯ç»„è£…
â”‚   â”‚       â”œâ”€â”€ save_history.rs  # ä¿å­˜å†å²æ¶ˆæ¯
â”‚   â”‚       â””â”€â”€ error_log.rs     # é”™è¯¯æ—¥å¿—
â”‚   â”œâ”€â”€ request.rs             # OpenAI è¯·æ±‚å°è£…
â”‚   â”œâ”€â”€ response.rs            # OpenAI å“åº”å°è£…
â”‚   â”œâ”€â”€ unified_message.rs     # ç»Ÿä¸€æ¶ˆæ¯æ ¼å¼ï¼ˆæ–‡æœ¬/å¤šæ¨¡æ€è½¬æ¢ï¼‰
â”‚   â”œâ”€â”€ types.rs               # æ ¸å¿ƒç±»å‹å®šä¹‰
â”‚   â”œâ”€â”€ error.rs               # é”™è¯¯ç±»å‹å®šä¹‰
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ image.rs           # å›¾åƒå¤„ç†å·¥å…·ï¼ˆBase64/URL/æœ¬åœ°æ–‡ä»¶ï¼‰
â”‚       â”œâ”€â”€ log.rs             # æ—¥å¿—åˆå§‹åŒ–
â”‚       â””â”€â”€ mod.rs
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ text_generation.rs             # æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹
â”‚   â”œâ”€â”€ vision_generation.rs           # è§†è§‰æ¨ç†ç¤ºä¾‹
â”‚   â”œâ”€â”€ vision_generation_stream.rs    # æµå¼è§†è§‰æ¨ç†
â”‚   â”œâ”€â”€ vision_parallel_generation.rs  # å¹¶å‘æ¨ç†ç¤ºä¾‹
â”‚   â””â”€â”€ check_gpu.rs                   # GPU æ£€æµ‹
â””â”€â”€ Cargo.toml
```

## å¿«é€Ÿå¼€å§‹

### ä¾èµ–

```toml
[dependencies]
llama_flow = { path = "apps/llama-flow" }
tokio = { version = "1", features = ["full"] }
```

### 1. çº¯æ–‡æœ¬æ¨ç†

```rust
use llama_flow::{
    Pipeline, PipelineConfig,
    request::{ChatMessagesBuilder, UserMessageBuilder, CreateChatCompletionRequestArgs},
    response::response_extract_content,
};

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    // é…ç½®æµæ°´çº¿
    let pipeline_config = PipelineConfig::new("/path/to/model.gguf")
        .with_n_gpu_layers(10)      // GPU å±‚æ•°
        .with_n_ctx(4096)            // ä¸Šä¸‹æ–‡çª—å£
        .with_temperature(0.7);      // é‡‡æ ·æ¸©åº¦

    let pipeline = Pipeline::try_new(pipeline_config)?;

    // æ„å»ºæ¶ˆæ¯ï¼ˆOpenAI å…¼å®¹ï¼‰
    let messages = ChatMessagesBuilder::new()
        .system("You are a helpful assistant.")
        .user("Who won the world series in 2020?")
        .assistant("The Los Angeles Dodgers won the World Series in 2020.")
        .user("Where was it played?")
        .build();

    // åˆ›å»ºè¯·æ±‚
    let request = CreateChatCompletionRequestArgs::default()
        .max_completion_tokens(2048u32)
        .model("model-name")
        .messages(messages)
        .build()?;

    // æ‰§è¡Œæ¨ç†
    let response = pipeline.generate(&request).await?;
    println!("Response: {}", response_extract_content(&response));
    Ok(())
}
```

### 2. è§†è§‰æ¨ç†ï¼ˆå¤šæ¨¡æ€ï¼‰

```rust
use base64::Engine;
use llama_flow::{
    Pipeline, PipelineConfig,
    request::{ChatMessagesBuilder, UserMessageBuilder, Metadata, CreateChatCompletionRequestArgs},
    response::response_extract_content,
};

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    // é…ç½®å¤šæ¨¡æ€æµæ°´çº¿
    let pipeline_config = PipelineConfig::new_with_mmproj(
        "/path/to/vision-model.gguf",
        "/path/to/mmproj.gguf"
    )
    .with_n_gpu_layers(10)
    .with_media_marker("<start_of_image>")  // åª’ä½“æ ‡è®°
    .with_image_max_resolution(768);         // å›¾åƒæœ€å¤§åˆ†è¾¨ç‡

    let pipeline = Pipeline::try_new(pipeline_config)?;

    // è¯»å–å›¾ç‰‡å¹¶ç¼–ç ä¸º Base64
    let image_data = std::fs::read("/path/to/image.png")?;
    let base64_data = base64::engine::general_purpose::STANDARD.encode(&image_data);
    
    // è‡ªåŠ¨æ£€æµ‹ MIME ç±»å‹
    let mime_type = infer::get_from_path("/path/to/image.png")
        .ok()
        .flatten()
        .map(|t| t.mime_type().to_string())
        .unwrap_or_else(|| "image/png".to_string());

    // æ„å»ºå¤šæ¨¡æ€æ¶ˆæ¯ï¼ˆæ–‡æœ¬ + å›¾ç‰‡ï¼‰
    let messages = ChatMessagesBuilder::new()
        .system("You are a helpful assistant.")
        .users(
            UserMessageBuilder::new()
                .text("Describe this image")
                .image_base64(mime_type, base64_data)  // Base64 å›¾ç‰‡
                // æˆ–ä½¿ç”¨ .image_url("https://...")   // è¿œç¨‹ URL
        )
        .build();

    // ä¼šè¯ç®¡ç†ï¼ˆå¯é€‰ï¼‰
    let metadata = Metadata {
        session_id: Some("12345".to_string()),
        ..Default::default()
    };

    let request = CreateChatCompletionRequestArgs::default()
        .max_completion_tokens(2048u32)
        .model("vision-model")
        .metadata(metadata)
        .messages(messages)
        .build()?;

    let response = pipeline.generate(&request).await?;
    println!("Response: {}", response_extract_content(&response));
    Ok(())
}
```

### 3. æµå¼æ¨ç†

```rust
use futures::StreamExt;
use llama_flow::Pipeline;

// æµå¼ç”Ÿæˆï¼ˆè¿”å› Stream<CreateChatCompletionStreamResponse>ï¼‰
let mut stream = pipeline.generate_stream(&request).await?;

while let Some(chunk) = stream.next().await {
    match chunk {
        Ok(response) => {
            if let Some(choice) = response.choices.first() {
                if let Some(content) = &choice.delta.content {
                    print!("{}", content);
                }
            }
        }
        Err(e) => eprintln!("Error: {}", e),
    }
}
```

### 4. é«˜çº§é…ç½®

```rust
use llama_flow::PipelineConfig;

let config = PipelineConfig::new("model.gguf")
    // æ¨¡å‹é…ç½®
    .with_mmproj_path("mmproj.gguf")    // å¤šæ¨¡æ€æŠ•å½±æ–‡ä»¶
    .with_n_gpu_layers(33)               // GPU å±‚æ•°ï¼ˆ0 = çº¯ CPUï¼‰
    .with_main_gpu(0)                    // ä¸» GPU è®¾å¤‡ ID
    .with_threads(8)                     // CPU çº¿ç¨‹æ•°
    
    // ä¸Šä¸‹æ–‡é…ç½®
    .with_n_ctx(8192)                    // ä¸Šä¸‹æ–‡çª—å£å¤§å°
    .with_n_batch(512)                   // æ‰¹å¤„ç†å¤§å°
    .with_max_completion_tokens(2048)    // æœ€å¤§ç”Ÿæˆ token æ•°
    .with_max_history(20)                // æœ€å¤§å†å²æ¶ˆæ¯æ•°
    
    // é‡‡æ ·å‚æ•°
    .with_temperature(0.7)               // æ¸©åº¦ï¼ˆ0.0-2.0ï¼‰
    .with_top_k(40)                      // Top-K é‡‡æ ·
    .with_top_p(0.95)                    // Top-Pï¼ˆnucleusï¼‰é‡‡æ ·
    .with_seed(42)                       // éšæœºç§å­
    
    // å¤šæ¨¡æ€é…ç½®
    .with_media_marker("<image>")        // åª’ä½“æ ‡è®°
    .with_image_max_resolution(1024)     // å›¾åƒæœ€å¤§åˆ†è¾¨ç‡
    
    // è°ƒè¯•é€‰é¡¹
    .with_verbose(false);                // è¯¦ç»†æ—¥å¿—
```

## æ ¸å¿ƒæ¦‚å¿µ

### 1. Pipelineï¼ˆæ¨ç†æµæ°´çº¿ï¼‰

`Pipeline` æ˜¯æ¨ç†çš„æ ¸å¿ƒå…¥å£ï¼Œå°è£…äº†å®Œæ•´çš„æ¨ç†æµç¨‹ï¼š

- **æ¨¡å‹åŠ è½½**ï¼šæ”¯æŒæ¨¡å‹ç¼“å­˜å’Œå¤ç”¨
- **ä¸Šä¸‹æ–‡ç®¡ç†**ï¼šè‡ªåŠ¨ç®¡ç†æ–‡æœ¬/å¤šæ¨¡æ€ä¸Šä¸‹æ–‡
- **é’©å­æ‰§è¡Œ**ï¼šåœ¨æ¨ç†å„é˜¶æ®µæ‰§è¡Œè‡ªå®šä¹‰é€»è¾‘
- **å“åº”ç”Ÿæˆ**ï¼šæ”¯æŒæµå¼å’Œéæµå¼è¾“å‡º

```rust
// Pipeline å¯ä»¥è¢« Arc åŒ…è£…ï¼Œå®‰å…¨åœ°åœ¨å¤šä¸ªå¼‚æ­¥ä»»åŠ¡é—´å…±äº«
let pipeline = Arc::new(Pipeline::try_new(config)?);

// éæµå¼æ¨ç†
let response = pipeline.generate(&request).await?;

// æµå¼æ¨ç†
let stream = pipeline.generate_stream(&request).await?;
```

### 2. è¯·æ±‚æ„å»ºå™¨ï¼ˆOpenAI å…¼å®¹ï¼‰

ä½¿ç”¨ `ChatMessagesBuilder` å’Œ `UserMessageBuilder` æ„å»ºç¬¦åˆ OpenAI æ ‡å‡†çš„è¯·æ±‚ï¼š

```rust
// çº¯æ–‡æœ¬æ¶ˆæ¯
let messages = ChatMessagesBuilder::new()
    .system("You are a helpful assistant.")
    .user("Hello!")
    .build();

// å¤šæ¨¡æ€æ¶ˆæ¯ï¼ˆæ–‡æœ¬ + å›¾ç‰‡ï¼‰
let messages = ChatMessagesBuilder::new()
    .system("You are a helpful assistant.")
    .users(
        UserMessageBuilder::new()
            .text("Describe this image")
            .image_url("https://example.com/image.jpg")
            .image_base64("image/png", base64_data)
    )
    .build();
```

### 3. ä¼šè¯ç®¡ç†

é€šè¿‡ `Metadata.session_id` å®ç°å¤šä¼šè¯éš”ç¦»ï¼š

```rust
use llama_flow::request::Metadata;

let metadata = Metadata {
    session_id: Some("user-123".to_string()),
    ..Default::default()
};

let request = CreateChatCompletionRequestArgs::default()
    .metadata(metadata)
    .messages(messages)
    .build()?;
```

æ¯ä¸ª session ç‹¬ç«‹ç»´æŠ¤ï¼š

- å†å²æ¶ˆæ¯è®°å½•
- ä¸Šä¸‹æ–‡çŠ¶æ€

### 4. é’©å­ç³»ç»Ÿ

é’©å­ç³»ç»Ÿæä¾›çµæ´»çš„æ‰©å±•ç‚¹ï¼Œå†…ç½®é’©å­åŒ…æ‹¬ï¼š

- **validate**ï¼šè¯·æ±‚å‚æ•°éªŒè¯
- **normalize**ï¼šæ¶ˆæ¯æ ¼å¼å½’ä¸€åŒ–
- **system_prompt**ï¼šç³»ç»Ÿæç¤ºè¯å¤„ç†
- **load_history**ï¼šä»ä¼šè¯åŠ è½½å†å²æ¶ˆæ¯
- **assemble_messages**ï¼šç»„è£…æœ€ç»ˆè¾“å…¥æ¶ˆæ¯
- **save_history**ï¼šä¿å­˜æ¨ç†ç»“æœåˆ°å†å²
- **error_log**ï¼šé”™è¯¯æ—¥å¿—è®°å½•

é’©å­æŒ‰ä¼˜å…ˆçº§é¡ºåºæ‰§è¡Œï¼Œæ”¯æŒè‡ªå®šä¹‰æ‰©å±•ã€‚

### 5. æ¨¡å‹ç¼“å­˜

å…¨å±€ç¼“å­˜ç®¡ç†å™¨è‡ªåŠ¨ç®¡ç†æ¨¡å‹å®ä¾‹ï¼š

```rust
use llama_flow::cache::global_cache;

// è‡ªåŠ¨ç¼“å­˜å’Œå¤ç”¨
let model = global_cache().get_or_load(&config)?;

// æ‰‹åŠ¨æ¸…ç†ç¼“å­˜
global_cache().clear();
```

ç¼“å­˜ Key åŸºäºï¼š

- æ¨¡å‹è·¯å¾„
- mmproj è·¯å¾„
- ä¸»è¦é…ç½®å‚æ•°ï¼ˆGPU å±‚æ•°ã€çº¿ç¨‹æ•°ç­‰ï¼‰

## ç¤ºä¾‹

è¿è¡Œç¤ºä¾‹ï¼š

```bash
# æ–‡æœ¬ç”Ÿæˆ
cargo run --package llama-flow --example text_generation

# è§†è§‰æ¨ç†
cargo run --package llama-flow --example vision_generation

# æµå¼è§†è§‰æ¨ç†
cargo run --package llama-flow --example vision_generation_stream

# å¹¶å‘æ¨ç†
cargo run --package llama-flow --example vision_parallel_generation

# GPU æ£€æµ‹
cargo run --package llama-flow --example check_gpu --features vulkan

# è¿è¡Œæµ‹è¯•
cargo test --package llama-flow --lib

# GPU æ¨¡å¼è¿è¡Œï¼ˆVulkanï¼‰
cargo run --package llama-flow --example text_generation --features vulkan
cargo run --package llama-flow --example vision_generation --features vulkan
```

## ç¼–è¯‘

### Rust ç¼–è¯‘

ç¡®ä¿ä½ å·²ç»å®‰è£…äº† Rust å’Œ Cargoã€‚ç„¶åå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ç¼–è¯‘é¡¹ç›®ï¼š

```bash
# åŸºç¡€ç¼–è¯‘ï¼ˆCPU æ¨¡å¼ï¼‰
cargo build -p llama-flow

# å‘å¸ƒç¼–è¯‘ï¼ˆä¼˜åŒ–ï¼‰
cargo build -p llama-flow --release

# Vulkan GPU åŠ é€Ÿ
cargo build -p llama-flow --features vulkan --release

# CUDA GPU åŠ é€Ÿï¼ˆéœ€è¦ NVIDIA GPU å’Œ CUDA å·¥å…·é“¾ï¼‰
cargo build -p llama-flow --features cuda --release
```

### åŠŸèƒ½ç‰¹æ€§ï¼ˆFeaturesï¼‰

- **é»˜è®¤**ï¼šCPU æ¨¡å¼ï¼ŒåŠ¨æ€é“¾æ¥ llama.cpp
- **`vulkan`**ï¼šå¯ç”¨ Vulkan GPU åŠ é€Ÿï¼ˆè·¨å¹³å°ï¼Œæ”¯æŒ AMD/Intel/ç§»åŠ¨è®¾å¤‡ï¼‰
- **`cuda`**ï¼šå¯ç”¨ CUDA GPU åŠ é€Ÿï¼ˆä»… NVIDIAï¼‰

```toml
[dependencies]
llama_flow = { path = "apps/llama-flow", features = ["vulkan"] }
```

### å®‰å“ç¼–è¯‘

- **æ–¹æ³•ä¸€ï¼šä½¿ç”¨è„šæœ¬ç¼–è¯‘**

```sh
cd apps/llama-flow

# å®‰è£… cargo-make
cargo install cargo-make

# ç¼–è¯‘ Android ç‰ˆæœ¬
cargo make dev-android
```

- **æ–¹æ³•äºŒï¼šæ‰‹åŠ¨è®¾ç½®ç¯å¢ƒå˜é‡ç¼–è¯‘**

```sh
# è®¾ç½® Android NDK è·¯å¾„
export ANDROID_NDK=$NDK_HOME
export NDK_ROOT=$NDK_HOME
export ANDROID_NDK_ROOT=$NDK_HOME
 
# C ç¼–è¯‘å™¨
export CC_aarch64_linux_android=$NDK_HOME/toolchains/llvm/prebuilt/linux-x86_64/bin/aarch64-linux-android35-clang
export CC_armv7_linux_androideabi=$NDK_HOME/toolchains/llvm/prebuilt/linux-x86_64/bin/armv7a-linux-androideabi35-clang
export CC_x86_64_linux_android=$NDK_HOME/toolchains/llvm/prebuilt/linux-x86_64/bin/x86_64-linux-android35-clang
export CC_i686_linux_android=$NDK_HOME/toolchains/llvm/prebuilt/linux-x86_64/bin/i686-linux-android35-clang

# C++ ç¼–è¯‘å™¨
export CXX_aarch64_linux_android=$NDK_HOME/toolchains/llvm/prebuilt/linux-x86_64/bin/aarch64-linux-android35-clang++
export CXX_armv7_linux_androideabi=$NDK_HOME/toolchains/llvm/prebuilt/linux-x86_64/bin/armv7a-linux-androideabi35-clang++
export CXX_x86_64_linux_android=$NDK_HOME/toolchains/llvm/prebuilt/linux-x86_64/bin/x86_64-linux-android35-clang++
export CXX_i686_linux_android=$NDK_HOME/toolchains/llvm/prebuilt/linux-x86_64/bin/i686-linux-android35-clang++

# ç¼–è¯‘å„æ¶æ„
cargo build -p llama-flow --target aarch64-linux-android
cargo build -p llama-flow --target x86_64-linux-android
cargo build -p llama_flow --target i686-linux-android

# æ³¨æ„ï¼š32 ä½ armv7 æ¶æ„æ”¯æŒä¸å®Œæ•´ï¼Œbindgen åœ¨ 32 ä½ç³»ç»Ÿä¸Šæœ‰é—®é¢˜
# cargo build -p llama-flow --target armv7-linux-androideabi
```

**æ³¨æ„**ï¼šç¯å¢ƒå˜é‡éœ€è¦å•ç‹¬ exportï¼Œä¸èƒ½åœ¨å‘½ä»¤è¡Œä¸­ä¸€æ¬¡æ€§ä¼ é€’ï¼ˆä¼šå¤±è´¥ï¼‰ã€‚

## API å‚è€ƒ

### PipelineConfig

æµæ°´çº¿é…ç½®ä¸»è¦æ–¹æ³•ï¼š

| æ–¹æ³• | è¯´æ˜ | é»˜è®¤å€¼ |
| ------ | ------ | -------- |
| `new(model_path)` | åˆ›å»ºé…ç½®ï¼ˆæ–‡æœ¬æ¨¡å¼ï¼‰ | - |
| `new_with_mmproj(model, mmproj)` | åˆ›å»ºé…ç½®ï¼ˆå¤šæ¨¡æ€ï¼‰ | - |
| `with_n_gpu_layers(n)` | GPU å±‚æ•°ï¼ˆ0=CPUï¼‰ | 0 |
| `with_n_ctx(size)` | ä¸Šä¸‹æ–‡çª—å£å¤§å° | 4096 |
| `with_n_batch(size)` | æ‰¹å¤„ç†å¤§å° | 512 |
| `with_temperature(t)` | é‡‡æ ·æ¸©åº¦ | 0.6 |
| `with_top_k(k)` | Top-K é‡‡æ · | 40 |
| `with_top_p(p)` | Top-P é‡‡æ · | 0.95 |
| `with_threads(n)` | CPU çº¿ç¨‹æ•° | è‡ªåŠ¨ |
| `with_max_completion_tokens(n)` | æœ€å¤§ç”Ÿæˆ token æ•° | 512 |
| `with_media_marker(marker)` | åª’ä½“æ ‡è®°ï¼ˆå¤šæ¨¡æ€ï¼‰ | `<image>` |
| `with_image_max_resolution(px)` | å›¾åƒæœ€å¤§åˆ†è¾¨ç‡ | 1024 |
| `with_verbose(bool)` | è¯¦ç»†æ—¥å¿— | false |

### Pipeline

æ¨ç†æ¥å£ï¼š

```rust
impl Pipeline {
    /// åˆ›å»ºæµæ°´çº¿å®ä¾‹
    pub fn try_new(config: PipelineConfig) -> Result<Self, Error>;
    
    /// éæµå¼æ¨ç†
    pub async fn generate(
        &self, 
        request: &CreateChatCompletionRequest
    ) -> Result<CreateChatCompletionResponse, Error>;
    
    /// æµå¼æ¨ç†
    pub async fn generate_stream(
        &self, 
        request: &CreateChatCompletionRequest
    ) -> Result<impl Stream<Item = Result<CreateChatCompletionStreamResponse, Error>>, Error>;
}
```

### è¯·æ±‚æ„å»º

```rust
// ChatMessagesBuilder
impl ChatMessagesBuilder {
    pub fn new() -> Self;
    pub fn system(self, message: impl Into<String>) -> Self;
    pub fn user(self, message: impl Into<String>) -> Self;
    pub fn users(self, builder: UserMessageBuilder) -> Self;
    pub fn assistant(self, message: impl Into<String>) -> Self;
    pub fn build(self) -> Vec<ChatCompletionRequestMessage>;
}

// UserMessageBuilderï¼ˆå¤šæ¨¡æ€æ¶ˆæ¯ï¼‰
impl UserMessageBuilder {
    pub fn new() -> Self;
    pub fn text(self, text: impl Into<String>) -> Self;
    pub fn image_url(self, url: impl Into<String>) -> Self;
    pub fn image_base64(self, mime_type: impl Into<String>, data: impl Into<String>) -> Self;
    pub fn image_file(self, path: impl AsRef<Path>) -> Result<Self, Error>;
    pub fn build(self) -> ChatCompletionRequestUserMessage;
}
```

## æœ€ä½³å®è·µ

### 1. æ€§èƒ½ä¼˜åŒ–

**GPU åŠ é€Ÿ**ï¼š

- æ ¹æ®æ˜¾å­˜è°ƒæ•´ `n_gpu_layers`ï¼ˆå»ºè®®ä» 10-20 å¼€å§‹æµ‹è¯•ï¼‰
- ä½¿ç”¨ Vulkan ç‰¹æ€§å¯åœ¨æ›´å¤šè®¾å¤‡ä¸ŠåŠ é€Ÿ

**æ‰¹å¤„ç†**ï¼š

- å¢å¤§ `n_batch` å¯æå‡ååé‡ï¼ˆæ¨è 512-2048ï¼‰
- æ³¨æ„æ˜¾å­˜å ç”¨

**çº¿ç¨‹é…ç½®**ï¼š

- CPU æ¨¡å¼ä¸‹ï¼Œ`n_threads` å»ºè®®è®¾ä¸ºç‰©ç†æ ¸å¿ƒæ•°
- GPU æ¨¡å¼ä¸‹ï¼Œçº¿ç¨‹æ•°å½±å“è¾ƒå°

### 2. ä¸Šä¸‹æ–‡ç®¡ç†

**ä¸Šä¸‹æ–‡çª—å£**ï¼š

```rust
// é•¿æ–‡æœ¬åœºæ™¯
.with_n_ctx(8192)
.with_max_history(50)

// å®æ—¶å¯¹è¯åœºæ™¯
.with_n_ctx(4096)
.with_max_history(10)
```

**ä¼šè¯éš”ç¦»**ï¼š

- å§‹ç»ˆä¸ºä¸åŒç”¨æˆ·è®¾ç½®ä¸åŒçš„ `session_id`
- ä¼šè¯å†å²è‡ªåŠ¨ç®¡ç†ï¼Œæ”¯æŒä¸Šä¸‹æ–‡è¿ç»­å¯¹è¯

### 3. å¤šæ¨¡æ€æ¨ç†

**å›¾åƒå¤„ç†**ï¼š

```rust
// è°ƒæ•´åˆ†è¾¨ç‡ä»¥å¹³è¡¡è´¨é‡å’Œæ€§èƒ½
.with_image_max_resolution(768)  // ä½æ˜¾å­˜
.with_image_max_resolution(1024) // å¹³è¡¡
.with_image_max_resolution(1536) // é«˜è´¨é‡
```

**åª’ä½“æ ‡è®°**ï¼š

- ç¡®ä¿ `media_marker` ä¸æ¨¡å‹è®­ç»ƒæ—¶ä¸€è‡´
- å¸¸è§æ ‡è®°ï¼š`<image>`ã€`<start_of_image>`ã€`<|image|>`

### 4. å¹¶å‘æ¨ç†

Pipeline æ”¯æŒå¹¶å‘å®‰å…¨ï¼š

```rust
let pipeline = Arc::new(Pipeline::try_new(config)?);

// å¹¶å‘å¤„ç†å¤šä¸ªè¯·æ±‚
let tasks: Vec<_> = requests.into_iter().map(|req| {
    let pipeline = Arc::clone(&pipeline);
    tokio::spawn(async move {
        pipeline.generate(&req).await
    })
}).collect();

let results = futures::future::join_all(tasks).await;
```

## æ³¨æ„äº‹é¡¹

1. **æ¨¡å‹æ–‡ä»¶**ï¼š
   - ç¡®ä¿æ¨¡å‹æ–‡ä»¶ä¸º GGUF æ ¼å¼
   - å¤šæ¨¡æ€éœ€è¦åŒæ—¶æä¾›æ¨¡å‹å’Œ mmproj æ–‡ä»¶

2. **æ˜¾å­˜ç®¡ç†**ï¼š
   - GPU å±‚æ•°è¶Šå¤šï¼Œæ˜¾å­˜å ç”¨è¶Šå¤§
   - å¤šä¼šè¯åœºæ™¯æ³¨æ„æ˜¾å­˜æº¢å‡º

3. **çº¿ç¨‹å®‰å…¨**ï¼š
   - Pipeline å¯å®‰å…¨åœ°åœ¨å¤šçº¿ç¨‹é—´å…±äº«ï¼ˆä½¿ç”¨ Arcï¼‰
   - æ¨¡å‹ç¼“å­˜æ˜¯å…¨å±€çº¿ç¨‹å®‰å…¨çš„

4. **åŠ¨æ€é“¾æ¥**ï¼š
   - é»˜è®¤ä½¿ç”¨åŠ¨æ€é“¾æ¥ llama.cpp
   - ç¡®ä¿è¿è¡Œæ—¶ç¯å¢ƒæœ‰å¯¹åº”çš„å…±äº«åº“ï¼ˆlibllama.so/dllï¼‰

5. **ä¼šè¯æŒä¹…åŒ–**ï¼š
   - å†å²æ¶ˆæ¯ç›®å‰å­˜å‚¨åœ¨å†…å­˜ä¸­
   - éœ€è¦æŒä¹…åŒ–å¯æ‰©å±• `ChatHistoryManager`

## è®¸å¯è¯

æœ¬é¡¹ç›®åŸºäº [llama-cpp-2](https://github.com/utilityai/llama-cpp-rs) æ„å»ºï¼Œéµå¾ªç›¸å…³å¼€æºè®¸å¯ã€‚
